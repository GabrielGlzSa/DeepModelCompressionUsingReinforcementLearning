{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JmDGFG2--t5z","executionInfo":{"status":"ok","timestamp":1649195721561,"user_tz":300,"elapsed":165,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"}}},"outputs":[],"source":["# !git clone https://github.com/GabrielGlzSa/ModelCompression.git\n","# %cd ModelCompression/\n","# !git checkout --track origin/CompressionV2\n","# !pip install Pillow \\\n","#     h5py \\\n","#     keras_preprocessing \\\n","#     matplotlib \\\n","#     mock \\\n","#     numpy \\\n","#     scipy \\\n","#     sklearn \\\n","#     pandas \\\n","#     future \\\n","#     portpicker \\\n","#     enum34 \\\n","#     tensorflow==2.6.2 \\\n","#     tensorflow_datasets==4.0.1\\\n","#     tensorflow_transform\\\n","#     tensorboard_plugin_profile\\\n","#     seaborn\\\n","#     pyparsing==2.4.7\\\n","#     tf_agents \\\n","#     tensorflow-model-optimization"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19551,"status":"ok","timestamp":1649195742514,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"},"user_tz":300},"id":"iDwFWUHjSfoe","outputId":"d17ddf77-4d8c-40e9-ab2a-7db129c8709d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/Colab Projects\n"," Adadeep.ipynb   \u001b[0m\u001b[01;34mdata\u001b[0m/                     \u001b[01;34mModelCompression\u001b[0m/\n"," \u001b[01;34mcheckpoints\u001b[0m/   'DQN Optimization.ipynb'  'Test compressors.ipynb'\n"]}],"source":["from google.colab import drive\n","import sys\n","\n","drive.mount('/content/drive/')\n","%cd ./drive/MyDrive/Colab Projects/\n","%ls\n","\n","sys.path.insert(0, './ModelCompression')\n","dataset = 'mnist'"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VtFjvWDHDi7s","executionInfo":{"status":"ok","timestamp":1649189781849,"user_tz":300,"elapsed":6161,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"}}},"outputs":[],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import logging\n","import pandas as pd\n","from IPython.display import clear_output\n","from utils import load_dataset\n","from CompressionTechniques import *\n","from replay_buffer import ReplayBuffer\n","from environments import *\n","from custom_layers import ROIEmbedding, ROIEmbedding1D\n","logging.basicConfig(level=logging.DEBUG, format='%(asctime)s -%(levelname)s - %(funcName)s -  %(message)s')"]},{"cell_type":"markdown","source":["# Agent "],"metadata":{"id":"sYB3tZhG_AFk"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"CdZeZXGcDrW_","executionInfo":{"status":"ok","timestamp":1649189781849,"user_tz":300,"elapsed":3,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"}}},"outputs":[],"source":["class DQNAgent:\n","    def __init__(self, name, state_shape, n_actions, epsilon=0.0, layer_type='fc'):\n","        \"\"\"A simple DQN agent\"\"\"\n","        self.name = name\n","        \n","        if layer_type=='fc':\n","          input = tf.keras.layers.Input(shape=(None, state_shape[-1]))\n","          x = ROIEmbedding1D(n_bins=[32, 16, 8 ,4, 2, 1])(input)\n","          x = tf.keras.layers.Dense(512, activation='relu')(x)\n","          output = tf.keras.layers.Dense(n_actions, activation=tf.keras.activations.linear)(x)\n","        else:          \n","          input = tf.keras.layers.Input(shape=(None, None, state_shape[-1]))\n","          x = tf.keras.layers.Conv2D(64, kernel_size=3)(input)\n","          x = tf.keras.layers.Conv2D(64, kernel_size=3)(x)\n","          x = ROIEmbedding(n_bins=[(4,4), (2,2), (1,1)])(x)\n","          x = tf.keras.layers.Dense(512, activation='relu')(x)\n","          output = tf.keras.layers.Dense(n_actions, activation=tf.keras.activations.linear)(x)\n","        self.model = tf.keras.Model(inputs=input, outputs=output, name=name)\n","        self.model.summary()\n","        self.weights = self.model.trainable_weights\n","        self.epsilon = epsilon\n","\n","    def get_symbolic_qvalues(self, state_t):\n","        \"\"\"takes agent's observation, returns qvalues. Both are tf Tensors\"\"\"\n","        qvalues = self.model(state_t)\n","        return qvalues\n","\n","    def get_qvalues(self, state_t):\n","        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n","        qvalues = self.model(state_t)\n","        return qvalues\n","\n","    def sample_actions(self, qvalues):\n","        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n","        epsilon = self.epsilon\n","        batch_size, n_actions = qvalues.shape\n","        random_actions = np.random.choice(n_actions, size=batch_size)\n","        best_actions = qvalues.argmax(axis=-1)\n","        should_explore = np.random.choice([0, 1], batch_size, p=[1 - epsilon, epsilon])\n","        return np.where(should_explore, random_actions, best_actions)\n"]},{"cell_type":"markdown","metadata":{"id":"eA9cjUjPDxNY"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"MsaHYVHADxOm"},"source":["# Evaluate agent"]},{"cell_type":"code","source":["\n","def evaluate_adadeep(env, conv_agent, fc_agent, n_games=1, greedy=True, t_max=10000):\n","    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n","    rewards = []\n","    acc = []\n","    weights = []\n","    infos = []\n","    for _ in range(n_games):\n","        s = env.reset()\n","        reward = 0\n","        df = pd.DataFrame()\n","        for k in range(len(env.layer_name_list)):\n","          next_layer_name = env.layer_name_list[env._layer_counter]\n","          layer = env.model.get_layer(next_layer_name)\n","          was_conv = False\n","          random_feature = s[np.random.choice(s.shape[0], size=1)]\n","          if isinstance(layer, tf.keras.layers.Conv2D):\n","              qvalues = conv_agent.get_qvalues(random_feature).numpy()\n","              action = qvalues.argmax(axis=-1)[0] if greedy else conv_agent.sample_actions(qvalues)[0]\n","          if isinstance(layer, tf.keras.layers.Dense):\n","              qvalues = fc_agent.get_qvalues(random_feature).numpy()\n","              action = qvalues.argmax(axis=-1)[0] if greedy else fc_agent.sample_actions(qvalues)[0]\n","\n","          new_s, r, done, info = env.step(action)\n","          s = env.get_current_state()\n","          if done:\n","              s = env.reset()\n","              break\n","\n","          row = {'state': s, 'action': action, 'reward': r, 'next_state': new_s, 'done':done, 'info':info}\n","          df = df.append(row, ignore_index = True)\n","\n","        # Calculate reward using stats before and after compression\n","        before_stats = df.iloc[0]['info']\n","        after_stats = df.iloc[-1]['info']\n","        reward = after_stats['weights_after']/before_stats['weights_before'] + after_stats['acc_after']\n","\n","        rewards.append(reward)\n","        acc.append(info['acc_after'])\n","        weights.append(info['weights_after'])\n","    print(infos)\n","    return np.mean(rewards), np.mean(acc), np.mean(weights)\n","    \n","def play_and_record_adadeep(conv_agent, fc_agent, env, conv_replay, fc_replay, dataset, n_steps=1, debug=False):\n","    \"\"\"\n","    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer.\n","    Whenever game ends, add record with done=True and reset the game.\n","    It is guaranteed that env has done=False when passed to this function.\n","\n","    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n","\n","    :returns: return sum of rewards over time\n","    \"\"\"\n","    # initial state\n","    s = env.reset()\n","    # Play the game for n_steps as per instructions above\n","    rewards = 0\n","    \n","    for it in range(n_steps):\n","        df = pd.DataFrame()\n","        for k in range(len(env.layer_name_list)):\n","          next_layer_name = env.layer_name_list[env._layer_counter]\n","          layer = env.model.get_layer(next_layer_name)\n","          was_conv = False\n","          if isinstance(layer, tf.keras.layers.Conv2D):\n","              random_image = s[np.random.choice(s.shape[0], size=1)]\n","              qvalues = conv_agent.get_qvalues(random_image).numpy()\n","              action = conv_agent.sample_actions(qvalues)[0]\n","              was_conv = True\n","          if isinstance(layer, tf.keras.layers.Dense):\n","            random_image = s[np.random.choice(s.shape[0], size=1)]\n","            qvalues = fc_agent.get_qvalues(random_image).numpy()\n","            action = fc_agent.sample_actions(qvalues)[0]\n","\n","          new_s, r, done, info = env.step(action)\n","\n","          info['was_conv'] = was_conv\n","\n","          if debug:\n","            print('Iteration: {} of {}'.format(it, n_steps), s.shape, action, r, new_s.shape, done, info)\n","\n","\n","          row = {'state': s, 'action': action, 'reward': r, 'next_state': new_s, 'done':done, 'info':info}\n","          df = df.append(row, ignore_index = True)\n","\n","          s = env.get_current_state()\n","          if done:\n","              s = env.reset()\n","              break\n","\n","        # Calculate reward using stats before and after compression\n","        before_stats = df.iloc[0]['info']\n","        after_stats = df.iloc[-1]['info']\n","\n","        reward = after_stats['weights_after']/before_stats['weights_before'] + after_stats['acc_after']\n","\n","        # Set the same reward for all actions.\n","        df['reward'] = reward\n","        \n","        for idx, row in df.iterrows():\n","          was_conv = row['info']['was_conv']\n","          if was_conv:\n","            for idx, feature in enumerate(row['state']):\n","              conv_replay.add(feature, row['action'], row['reward'], row['next_state'][idx], row['done'])\n","          else:\n","            for idx, feature in enumerate(row['state']):\n","              fc_replay.add(feature, row['action'], row['reward'], row['next_state'][idx], row['done'])\n","        \n","        fc_replay.save('./data/{}_adadeep_fc_replay.pkl'.format(dataset))\n","        conv_replay.save('./data/{}_adadeep_conv_replay.pkl'.format(dataset))\n","    return reward"],"metadata":{"id":"AKAqNGjYdaa5","executionInfo":{"status":"ok","timestamp":1649189782215,"user_tz":300,"elapsed":368,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# % rm -r ./data/full_model/test_mnist/"],"metadata":{"id":"N4VBJtfdEtEx","executionInfo":{"status":"ok","timestamp":1649189782215,"user_tz":300,"elapsed":2,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r9RqwdKCD1we"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"lUpQpUdZD1xo"},"source":["# Create environment"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":502},"executionInfo":{"elapsed":13877,"status":"error","timestamp":1649189796218,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"},"user_tz":300},"id":"Qdxi8xTrIx0Z","outputId":"e464e75b-c742-4228-9605-dec7fcfc28a8"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-04-05 20:16:23,914 -INFO - read_from_directory -  Load dataset info from ./data/mnist/3.0.1\n","2022-04-05 20:16:24,431 -INFO - read_from_directory -  Field info.citation from disk and from code do not match. Keeping the one from code.\n","2022-04-05 20:16:24,434 -INFO - download_and_prepare -  Reusing dataset mnist (./data/mnist/3.0.1)\n","2022-04-05 20:16:24,436 -INFO - as_dataset -  Constructing tf.data.Dataset for split ['train[:80%]', 'train[80%:]', 'test'], from ./data/mnist/3.0.1\n","2022-04-05 20:16:29,452 -INFO - read_from_directory -  Load dataset info from ./data/mnist/3.0.1\n","2022-04-05 20:16:29,466 -INFO - read_from_directory -  Field info.citation from disk and from code do not match. Keeping the one from code.\n","2022-04-05 20:16:29,473 -INFO - download_and_prepare -  Reusing dataset mnist (./data/mnist/3.0.1)\n","2022-04-05 20:16:29,479 -INFO - as_dataset -  Constructing tf.data.Dataset for split ['train[:80%]', 'train[80%:]', 'test'], from ./data/mnist/3.0.1\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-af965880926a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-af965880926a>\u001b[0m in \u001b[0;36mmake_env\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     58\u001b[0m     env = AdaDeepEnv(compressors_list, model_path, parameters,\n\u001b[1;32m     59\u001b[0m                  \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                  layer_name_list, input_shape, current_state_source='layer_input', next_state_source='layer_output')\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Projects/ModelCompression/environments.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, compressors_list, model_path, compr_params, dataset, validation, layer_name_list, input_shape, current_state_source, next_state_source)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompressionTechniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             temp_comp = class_(model=self.model, dataset=self.dataset, optimizer=self.optimizer, loss=self.loss_object, metrics=self.train_metric,\n\u001b[0;32m--> 213\u001b[0;31m                             fine_tune=True, input_shape=self.input_shape)\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtemp_comp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_layer_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'conv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_compressors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'bases'"]}],"source":["def make_env(dataset):\n","    train_ds, val_ds, test_ds, input_shape, num_classes = load_dataset(dataset)\n","    train_ds, valid_ds, test_ds, input_shape, num_classes = load_dataset(dataset)\n","\n","    model_path = './data/full_model/test_'+dataset\n","    try:\n","      model = tf.keras.models.load_model(model_path, compile=True)\n","    except OSError:\n","      optimizer = tf.keras.optimizers.Adam()\n","      loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n","      train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n","      model = tf.keras.Sequential([tf.keras.layers.Conv2D(32, (3, 3), activation='relu', name='conv2d_0',\n","                                                          input_shape=input_shape),\n","                                    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', name='conv2d_1'),\n","                                    tf.keras.layers.MaxPool2D((2, 2), 2),\n","                                    tf.keras.layers.Flatten(),\n","                                    tf.keras.layers.Dense(128, activation='relu', name='dense_0'),\n","                                    tf.keras.layers.Dense(128, activation='relu', name='dense_1'),\n","                                    tf.keras.layers.Dense(num_classes, activation='softmax', name='dense_softmax')\n","                                    ])\n","      model.compile(optimizer=optimizer, loss=loss_object, metrics=train_metric)\n","      model.fit(train_ds, epochs=5, validation_data=valid_ds)\n","      model.save(model_path)\n","\n","#     model = tf.keras.applications.VGG16(\n","#     include_top=True,\n","#     weights=\"imagenet\",\n","#     input_tensor=None,\n","#     input_shape=None,\n","#     pooling=None,\n","#     classes=1000,\n","#     classifier_activation=\"softmax\",\n","# )\n","\n","    w_comprs = ['InsertDenseSVD', 'InsertDenseSparse', 'DeepCompression'] # 'InsertDenseSVDCustom'\n","    c_comprs = ['InsertSVDConv','DepthwiseSeparableConvolution', 'SparseConnectionsCompression'] #SparseConvolutionCompression\n","    l_comprs = ['FireLayerCompression', 'MLPCompression','ReplaceDenseWithGlobalAvgPool']\n","    compressors_list = w_comprs + c_comprs + l_comprs\n","\n","    parameters = {}\n","    parameters['DeepCompression'] = {'layer_name': 'dense_0', 'threshold': 0.001}\n","    parameters['ReplaceDenseWithGlobalAvgPool'] = {'layer_name': 'dense_1'}\n","    parameters['InsertDenseSVD'] = {'layer_name': 'dense_0', 'units': 16}\n","    parameters['InsertDenseSVDCustom'] = {'layer_name': 'dense_0', 'units': 16}\n","    parameters['InsertDenseSparse'] = {'layer_name': 'dense_0', 'verbose': True, 'units': 16}\n","    parameters['InsertSVDConv'] = {'layer_name': 'conv2d_1', 'units': 8}\n","    parameters['DepthwiseSeparableConvolution'] = {'layer_name': 'conv2d_1'}\n","    parameters['FireLayerCompression'] = {'layer_name': 'conv2d_1'}\n","    # parameters['MLPCompression'] = {'layer_name': 'conv2d_1'}\n","    parameters['SparseConnectionsCompression'] = {'layer_name': 'conv2d_1', 'epochs': 20,\n","                                                  'target_perc': 0.75, 'conn_perc_per_epoch': 0.1}\n","\n","    layer_name_list = ['conv2d_1','dense_0', 'dense_1']\n","    # env = LayerEnv(compressors_list, model_path, parameters,\n","    #              train_ds, val_ds,\n","    #              layer_name_list, input_shape, features_type='input_shape')\n","\n","    env = AdaDeepEnv(compressors_list, model_path, parameters,\n","                 train_ds, val_ds,\n","                 layer_name_list, input_shape, current_state_source='layer_input', next_state_source='layer_output')\n","    \n","    return env\n","\n","env = make_env(dataset)\n","env.model.summary()"]},{"cell_type":"markdown","metadata":{"id":"0sPJTrzJI2s-"},"source":["# Create DQN for model compression\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":166,"status":"aborted","timestamp":1649189796216,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"},"user_tz":300},"id":"WSZAzi-RDnRr"},"outputs":[],"source":["fc_state_dim = (1,)\n","fc_n_actions = len(env.dense_compressors)\n","conv_state_dim = list(env.get_current_state().shape)[1:]\n","conv_n_actions = len(env.conv_compressors)\n","\n","fc_agent = DQNAgent(\"dqn_agent_fc\", fc_state_dim, fc_n_actions, epsilon=0.9, layer_type='fc')\n","fc_target_network = DQNAgent(\"target_network_fc\", fc_state_dim, fc_n_actions, layer_type='fc')\n","\n","\n","conv_agent = DQNAgent(\"dqn_agent_conv\", conv_state_dim, conv_n_actions, epsilon=0.9, layer_type='cnn')\n","conv_target_network = DQNAgent(\"target_network_conv\", conv_state_dim, conv_n_actions, layer_type='cnn')\n","\n","def load_weigths_into_target_network(agent, target_network):\n","    \"\"\" assign target_network.weights variables to their respective agent.weights values. \"\"\"\n","    for i in range(len(agent.model.layers)):\n","        target_network.model.layers[i].set_weights(agent.model.layers[i].get_weights())\n","\n","\n","load_weigths_into_target_network(fc_agent, fc_target_network)\n","load_weigths_into_target_network(conv_agent, conv_target_network)\n","\n","for w, w2 in zip(fc_agent.weights, fc_target_network.weights):\n","    tf.assert_equal(w, w2)\n","for w, w2 in zip(conv_agent.weights, conv_target_network.weights):\n","    tf.assert_equal(w, w2)\n","\n","print(\"It works!\")"]},{"cell_type":"markdown","source":["# Training func\n"],"metadata":{"id":"30sGbVmEaXDS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNXKajHxJ8Jx","executionInfo":{"status":"aborted","timestamp":1649189796217,"user_tz":300,"elapsed":166,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"}}},"outputs":[],"source":["def moving_average(x, span=100, **kw):\n","    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span, **kw).mean().values\n","\n","\n","mean_weights_history = []\n","mean_acc_history = []\n","mean_rw_history = []\n","td_loss_history = []\n","\n","\n","\n","\n","def sample_batch(exp_replay, batch_size):\n","    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n","    return {\n","        'state': obs_batch,\n","        'action': act_batch,\n","        'rewards': reward_batch,\n","        'next_state': next_obs_batch,\n","        'done': is_done_batch,\n","    }\n","\n","\n","@tf.function\n","def training_loop(state, action, rewards, next_state, done, agent, target_agent, loss_optimizer, n_actions, gamma=0.99):\n","    state = tf.cast(state, tf.float32)\n","    action = tf.cast(action, tf.int32)\n","    next_state = tf.cast(next_state, tf.float32)\n","\n","    rewards = tf.cast(rewards, tf.float32)\n","    done = 1 - tf.cast(done, tf.float32)\n","\n","    reference_qvalues = rewards + gamma * tf.reduce_max(target_agent.get_qvalues(next_state), axis=1)\n","    reference_qvalues = reference_qvalues * (1 - done) - done\n","\n","    masks = tf.one_hot(action, n_actions)\n","    with tf.GradientTape() as tape:\n","        q_values = agent.get_qvalues(state)\n","        q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n","        td_loss = tf.reduce_mean((q_action - reference_qvalues) ** 2)\n","\n","    gradients = tape.gradient(td_loss, agent.weights)\n","    loss_optimizer.apply_gradients(zip(gradients, agent.weights))\n","    return td_loss\n","\n"]},{"cell_type":"markdown","source":["# Generate some replay buffer samples"],"metadata":{"id":"XKY9uyNaV4Gf"}},{"cell_type":"code","source":["\n","fc_agent.epsilon = 0.9\n","conv_agent.epsilon = 0.9\n","min_epsilon = 0.1\n","optimizer = tf.keras.optimizers.Adam(1e-5)\n","iterations = 1000\n","\n","fc_exp_replay = ReplayBuffer(10 ** 5)\n","conv_exp_replay = ReplayBuffer(10 ** 5)\n","\n","try:\n","  fc_exp_replay.load('./data/{}_adadeep_fc_replay.pkl'.format(dataset))\n","  conv_exp_replay.load('./data/{}_adadeep_conv_replay.pkl'.format(dataset))\n","except FileNotFoundError:\n","  pass\n","\n","print('There are {} conv and {} fc instances.'.format(len(conv_exp_replay._storage),len(fc_exp_replay._storage)))\n","play_and_record_adadeep(conv_agent, fc_agent, env, conv_exp_replay, fc_exp_replay, dataset, 32, debug=True)\n","\n","print('There are {} conv and {} fc instances.'.format(len(conv_exp_replay._storage),len(fc_exp_replay._storage)))"],"metadata":{"id":"SodYaCp5V-bH","executionInfo":{"status":"aborted","timestamp":1649189796217,"user_tz":300,"elapsed":166,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RFL Training"],"metadata":{"id":"xG06jA42xer3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMnQZd5kbrtz","executionInfo":{"status":"aborted","timestamp":1649189796217,"user_tz":300,"elapsed":166,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"}}},"outputs":[],"source":["from tqdm import tqdm\n","logging.basicConfig(level=logging.WARNING, format='%(asctime)s -%(levelname)s - %(funcName)s -  %(message)s')\n","\n","with tqdm(total=iterations,\n","          bar_format=\"{l_bar}{bar}|{n}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {postfix[0]}: {postfix[1]:.4f} {postfix[2]}: {postfix[3][0]:.2f}, {postfix[3][1]:.2f} & {postfix[3][2]:.2f}]\",\n","          postfix=[\"Epsilon\", conv_agent.epsilon, 'Last 3 RW', dict({0:0,1:0,2:0})]) as t:\n","    for i in range(iterations):\n","        # play\n","        play_and_record_adadeep(conv_agent, fc_agent, env, conv_exp_replay, fc_exp_replay, dataset, 1, debug=True)\n","\n","        # train fc\n","        batch_data = sample_batch(conv_exp_replay, batch_size=32)\n","        batch_data['agent'] = conv_agent\n","        batch_data['target_agent'] = conv_target_network\n","        batch_data['loss_optimizer'] = optimizer\n","        batch_data['n_actions'] = conv_n_actions\n","        conv_loss_t = training_loop(**batch_data)\n","\n","        # train\n","        batch_data = sample_batch(fc_exp_replay, batch_size=32)\n","        batch_data['agent'] = fc_agent\n","        batch_data['target_agent'] = fc_target_network\n","        batch_data['loss_optimizer'] = optimizer\n","        batch_data['n_actions'] = fc_n_actions\n","        fc_loss_t = training_loop(**batch_data)\n","        td_loss_history.append(conv_loss_t+fc_loss_t)\n","\n","        # adjust agent parameters\n","        if i % 10 == 0:\n","            load_weigths_into_target_network(conv_agent, conv_target_network)\n","            conv_target_network.model.save_weights('./checkpoints/{}_my_checkpoint_conv'.format(dataset))\n","            conv_agent.epsilon = max(conv_agent.epsilon * 0.98, min_epsilon)\n","            t.postfix[1] = conv_agent.epsilon\n","\n","            load_weigths_into_target_network(fc_agent, fc_target_network)\n","            fc_target_network.model.save_weights('./checkpoints/{}_my_checkpoint_fc'.format(dataset))\n","            fc_agent.epsilon = conv_agent.epsilon\n","\n","            rw, acc, weights = evaluate_adadeep(make_env(dataset), conv_agent, fc_agent, n_games=3)\n","            mean_rw_history.append(rw)\n","            mean_acc_history.append(acc)\n","            mean_weights_history.append(weights)\n","            t.postfix[3][2] = mean_rw_history[-1]\n","            try:\n","                t.postfix[3][1] = mean_rw_history[-2]\n","            except IndexError:\n","                t.postfix[3][1] = 0\n","            try:\n","                t.postfix[3][0] = mean_rw_history[-3]\n","            except IndexError:\n","                t.postfix[3][0] = 0\n","        t.update()\n","\n","\n","        if i%100==0:\n","            clear_output(True)\n","            plt.subplot(1, 3, 1)\n","            plt.title(\"mean reward per game\")\n","            plt.plot(mean_rw_history)\n","            plt.grid()\n","\n","            assert not np.isnan(td_loss_history).any()\n","\n","            plt.subplot(1, 3, 2)\n","            plt.title(\"TD loss history (moving average)\")\n","            plt.plot(moving_average(np.array(td_loss_history), span=100, min_periods=100))\n","            plt.grid()\n","\n","            plt.subplot(1, 4, 3)\n","            plt.title(\"Weights history\")\n","            plt.plot(mean_weights_history)\n","            plt.grid()\n","\n","            plt.subplot(1, 4, 4)\n","            plt.title(\"Acc history)\")\n","            plt.plot(mean_acc_history)\n","            plt.grid()\n","            plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["sYB3tZhG_AFk","MsaHYVHADxOm","xG06jA42xer3"],"name":"Adadeep.ipynb","provenance":[],"authorship_tag":"ABX9TyO0z3TsNyPKddm2KeYV4yiJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}