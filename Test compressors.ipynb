{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":580,"status":"ok","timestamp":1649319460186,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"},"user_tz":300},"id":"JmDGFG2--t5z"},"outputs":[],"source":["# !git clone https://github.com/GabrielGlzSa/ModelCompression.git\n","# %cd ModelCompression/\n","# !git checkout --track origin/CompressionV2\n","# !pip install Pillow \\\n","#     h5py \\\n","#     keras_preprocessing \\\n","#     matplotlib \\\n","#     mock \\\n","#     numpy \\\n","#     scipy \\\n","#     sklearn \\\n","#     pandas \\\n","#     future \\\n","#     portpicker \\\n","#     enum34 \\\n","#     tensorflow==2.6.2 \\\n","#     tensorflow_datasets==4.0.1\\\n","#     tensorflow_transform\\\n","#     tensorboard_plugin_profile\\\n","#     seaborn\\\n","#     pyparsing==2.4.7\\\n","#     tf_agents \\\n","#     tensorflow-model-optimization\n","%load_ext tensorboard"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1823,"status":"ok","timestamp":1649319462727,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"},"user_tz":300},"id":"iDwFWUHjSfoe","outputId":"4b211ce4-d4bb-406a-a43f-999c76c2f4f5"},"outputs":[],"source":["# from google.colab import drive\n","# import sys\n","\n","# drive.mount('/content/drive/')\n","# %cd ./drive/MyDrive/Colab Projects/\n","# %ls\n","\n","sys.path.insert(0, './ModelCompression')\n","dataset = 'mnist'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4491,"status":"ok","timestamp":1649319467216,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"},"user_tz":300},"id":"VtFjvWDHDi7s"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/A00806415/anaconda3/envs/mc/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import logging\n","import pandas as pd\n","from IPython.display import clear_output\n","from utils import load_dataset\n","from CompressionTechniques import *\n","from replay_buffer import ReplayBuffer\n","from environments import *\n","from custom_layers import *\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s -%(levelname)s - %(funcName)s -  %(message)s')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649319467217,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"},"user_tz":300},"id":"mzVWlzxlH2Yg"},"outputs":[],"source":["# kernel_size = (3,3)\n","# channels = 3\n","# bases = 4\n","# filters = 32\n","# fh, fw = kernel_size\n","# S = np.random.random(size=(channels, bases, filters))\n","# Q = np.random.random(size=(channels, fh, fw, bases))\n","# P = np.random.random(size=(channels, channels))\n","# v1 = SparseConvolution2D(kernel_size=kernel_size,\n","#                            filters=filters,\n","#                            PQS=[P, Q, S],\n","#                            bases=bases)\n","\n","# imgs = tf.constant(np.random.randint(0, 255, size=(4,32,32,3)).astype(np.float32))\n","# %timeit v1(imgs)"]},{"cell_type":"markdown","metadata":{"id":"r9RqwdKCD1we"},"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":68502,"status":"error","timestamp":1649319535715,"user":{"displayName":"Gabriel González Sahagún","userId":"12497170045078186877"},"user_tz":300},"id":"Qdxi8xTrIx0Z","outputId":"c02f1d3c-76ee-4db8-d07d-0e8101c08859"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-05-16 14:05:51,140 -INFO - read_from_directory -  Load dataset info from ./data/mnist/3.0.1\n","2022-05-16 14:05:51,144 -INFO - download_and_prepare -  Reusing dataset mnist (./data/mnist/3.0.1)\n","2022-05-16 14:05:51,146 -INFO - as_dataset -  Constructing tf.data.Dataset for split ['train[:80%]', 'train[80%:]', 'test'], from ./data/mnist/3.0.1\n","2022-05-16 14:05:51,632 -INFO - compress_layer -  Searching for layer: conv2d_1\n","2022-05-16 14:05:51,641 -INFO - compress_layer -  Learning MLP filter.\n","2022-05-16 14:06:55,081 -INFO - fine_tune_model -  Fine-tuning the whole optimized model.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","1500/1500 [==============================] - 6s 4ms/step - loss: 0.0161 - sparse_categorical_accuracy: 0.9948\n","Epoch 2/2\n","1500/1500 [==============================] - 6s 4ms/step - loss: 0.0123 - sparse_categorical_accuracy: 0.9963\n"]},{"name":"stderr","output_type":"stream","text":["2022-05-16 14:07:07,404 -INFO - compress_layer -  Replaced layer was using 9248 weights.\n","2022-05-16 14:07:07,406 -INFO - compress_layer -  MLPConv is using 5168 weights.\n","2022-05-16 14:07:07,407 -INFO - compress_layer -  Finished compression\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n","_________________________________________________________________\n","conv2d_0 (Conv2D)            (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","conv2d_1/MLPConv (MLPConv)   (None, 24, 24, 32)        5168      \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 22, 22, 32)        9248      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 11, 11, 32)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 3872)              0         \n","_________________________________________________________________\n","dense_0 (Dense)              (None, 128)               495744    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","dense_softmax (Dense)        (None, 10)                1290      \n","=================================================================\n","Total params: 528,282\n","Trainable params: 528,282\n","Non-trainable params: 0\n","_________________________________________________________________\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["def make_env(dataset):\n","    train_ds, valid_ds, test_ds, input_shape, num_classes = load_dataset(dataset)\n","\n","    model_path = './data/full_model/test_'+dataset\n","    optimizer = tf.keras.optimizers.Adam()\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n","    train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n","    try:\n","      model = tf.keras.models.load_model(model_path, compile=True)\n","    except OSError:\n","      \n","      model = tf.keras.Sequential([tf.keras.layers.Conv2D(32, (3, 3), activation='relu', name='conv2d_0',\n","                                                          input_shape=input_shape),\n","                                   tf.keras.layers.Conv2D(32, (3, 3), activation='relu', name='conv2d_1'),\n","                                   tf.keras.layers.Conv2D(32, (3, 3), activation='relu', name='conv2d_2'),\n","                                   tf.keras.layers.MaxPool2D((2, 2), 2),\n","                                   tf.keras.layers.Flatten(),\n","                                   tf.keras.layers.Dense(128, activation='relu', name='dense_0'),\n","                                   tf.keras.layers.Dense(128, activation='relu', name='dense_1'),\n","                                   tf.keras.layers.Dense(num_classes, activation='softmax', name='dense_softmax')\n","                                   ])\n","      model.compile(optimizer=optimizer, loss=loss_object, metrics=train_metric)\n","      model.fit(train_ds, epochs=5, validation_data=valid_ds)\n","      model.save(model_path)\n","\n","    parameters = {}\n","    parameters['DeepCompression'] = {'layer_name': 'dense_0', 'threshold': 0.001}\n","    parameters['ReplaceDenseWithGlobalAvgPool'] = {'layer_name': 'dense_1'}\n","    parameters['InsertDenseSVD'] = {'layer_name': 'dense_0', 'units': 16}\n","    parameters['InsertDenseSVDCustom'] = {'layer_name': 'dense_0', 'units': 16}\n","    parameters['InsertDenseSparse'] = {'layer_name': 'dense_0', 'verbose': True, 'units': 16}\n","    parameters['InsertSVDConv'] = {'layer_name': 'conv2d_1', 'units': 8}\n","    parameters['DepthwiseSeparableConvolution'] = {'layer_name': 'conv2d_1'}\n","    parameters['FireLayerCompression'] = {'layer_name': 'conv2d_1'}\n","    # parameters['MLPCompression'] = {'layer_name': 'conv2d_1'}\n","    parameters['SparseConnectionsCompression'] = {'layer_name': 'conv2d_1', 'epochs': 20,\n","                                                  'target_perc': 0.75, 'conn_perc_per_epoch': 0.1}\n","    # compressor = ReplaceDenseWithGlobalAvgPool(model=model, dataset=train_ds,\n","    #                                            optimizer=optimizer, loss=loss_object, metrics=train_metric)\n","    # compressor.compress_layer()\n","\n","\n","    # compressor = InsertDenseSVD(model=model, dataset=train_ds,\n","    #                             optimizer=optimizer, loss=loss_object, metrics=train_metric, fine_tune=False)\n","    # compressor.compress_layer('dense_1', units=32)\n","\n","\n","    # compressor = InsertDenseSparse(model=model, dataset=train_ds,\n","    #                             optimizer=optimizer, loss=loss_object, metrics=train_metric)\n","    # compressor.compress_layer('dense_1', verbose=True)\n","\n","\n","    # compressor = InsertSVDConv(model=model, dataset=train_ds,\n","    #                             optimizer=optimizer, loss=loss_object, metrics=train_metric)\n","    # compressor.compress_layer('conv2d_1')\n","\n","    # compressor = DepthwiseSeparableConvolution(model=model, dataset=train_ds,\n","    #                             optimizer=optimizer, loss=loss_object, metrics=train_metric)\n","    # compressor.compress_layer('conv2d_1')\n","\n","    # compressor = FireLayerCompression(model=model, dataset=train_ds,\n","    #                                   optimizer=optimizer, loss=loss_object, metrics=train_metric)\n","    # compressor.compress_layer('conv2d_1')\n","\n","    compressor = MLPCompression(model=model, dataset=train_ds,\n","                                optimizer=optimizer, loss=loss_object, metrics=train_metric, input_shape=input_shape)\n","    compressor.compress_layer('conv2d_1')\n","\n","    # population=20,generations=100 takes 3 hours and 40 minutes\n","    # compressor = SparseConnectionsCompressionCustom(model=model,\n","    #                                                 dataset=train_ds,\n","    #                                                 optimizer=optimizer,\n","    #                                                 loss=loss_object,\n","    #                                                 metrics=train_metric)\n","    # compressor.compress_layer('conv2d_1', epochs=10)\n","\n","    # RAM OOM in training loop.\n","    # compressor = SparseConvolutionCompression(model=model, dataset=train_ds,\n","    #                             optimizer=optimizer, loss=loss_object, metrics=train_metric, bases=4, input_shape=input_shape)\n","    # compressor.compress_layer('conv2d_1', iterations=2)\n","\n","    new_model = compressor.get_model()\n","    new_model.summary()\n","\n","\n","make_env(dataset)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMad9ua7IxZlq3VjAmTHS8g","collapsed_sections":[],"name":"Test compressors.ipynb","provenance":[]},"interpreter":{"hash":"51ccaed8c3f743b1f0f573d1bf9b420e6968f0ad98d33b49a16252e93b5cad05"},"kernelspec":{"display_name":"Python 3.6.13 ('mc')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"}},"nbformat":4,"nbformat_minor":0}
